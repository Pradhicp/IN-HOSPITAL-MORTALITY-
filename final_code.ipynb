{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3711c1c",
   "metadata": {},
   "source": [
    "logistic regression with 5 fold cross validation and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4da372ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.7251618837825735\n",
      "Average Precision: 0.6671415331502913\n",
      "Average Recall: 0.8987979725830971\n",
      "Average F1 Score: 0.7655728053551304\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "\n",
    "# 70-30 Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Alpha value for regularization\n",
    "alpha_value = 0.001\n",
    "\n",
    "# Threshold for binary classification\n",
    "threshold_value = 0.3\n",
    "\n",
    "# Random state for KFold and SMOTE\n",
    "random_state_value = 123\n",
    "\n",
    "# Apply SMOTE for class imbalance\n",
    "smote = SMOTE(random_state=random_state_value)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Initialize Logistic Regression model with alpha\n",
    "log_reg_model = LogisticRegression(C=1/alpha_value)\n",
    "\n",
    "# Perform 5-fold cross-validation with SMOTE\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state_value)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_resampled):\n",
    "    X_train, X_test = X_resampled[train_index], X_resampled[test_index]\n",
    "    y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
    "    \n",
    "    # Train Logistic Regression model\n",
    "    log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities on test data\n",
    "    y_pred_proba = log_reg_model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on threshold\n",
    "    y_pred_binary = (y_pred_proba > threshold_value).astype(int)\n",
    "\n",
    "    # Evaluate Logistic Regression model\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb212e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a68715a",
   "metadata": {},
   "source": [
    "after improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac1cfb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8919438285291944\n",
      "Average Precision: 0.7216666666666666\n",
      "Average Recall: 0.24741200828157348\n",
      "Average F1 Score: 0.36600684261974586\n",
      "Confusion Matrix for Fold 1:\n",
      "[[139   3]\n",
      " [ 17   6]]\n",
      "Confusion Matrix for Fold 2:\n",
      "[[136   1]\n",
      " [ 21   7]]\n",
      "Confusion Matrix for Fold 3:\n",
      "[[138   6]\n",
      " [ 17   4]]\n",
      "Confusion Matrix for Fold 4:\n",
      "[[150   0]\n",
      " [ 10   4]]\n",
      "Confusion Matrix for Fold 5:\n",
      "[[146   2]\n",
      " [ 12   4]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# Split data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=20)\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrices = []  # List to store confusion matrices for each fold\n",
    "\n",
    "# Adjust the threshold for binary classification\n",
    "threshold = 0.9 # Adjust this threshold as needed\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Apply SMOTE to the training fold only\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Train Logistic Regression model\n",
    "    log_reg_model = LogisticRegression()  # No need to specify regularization parameters for now\n",
    "    log_reg_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict probabilities on validation fold\n",
    "    y_pred_proba = log_reg_model.predict_proba(X_val_fold)[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # Convert probabilities to binary predictions based on threshold\n",
    "    y_pred_binary = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "    # Evaluate Logistic Regression model\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred_binary))\n",
    "    \n",
    "    # Calculate the confusion matrix for the fold\n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred_binary)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "\n",
    "# Print confusion matrix for each fold\n",
    "for i, conf_matrix in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5246052",
   "metadata": {},
   "source": [
    "Lasso regression with 5 fold cross validation and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a0fe8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.7663324918860439\n",
      "Average Precision: 0.325191796737295\n",
      "Average Recall: 0.6538267364683572\n",
      "Average F1 Score: 0.4307915519689646\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection with Lasso regularization\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# 70-30 Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize Lasso Regression model\n",
    "lasso_model = Lasso(alpha=0.00005)  # Adjust regularization strength (alpha)\n",
    "\n",
    "# Suppress convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Perform 5-fold cross-validation with SMOTE\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=16)  # Change the random state value here\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train Lasso Regression model\n",
    "    lasso_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = lasso_model.predict(X_test)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "    # Evaluate Lasso Regression model\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5de7a",
   "metadata": {},
   "source": [
    "after improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d02e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8955875831485587\n",
      "Average Precision: 0.8228571428571427\n",
      "Average Recall: 0.20872153209109728\n",
      "Average F1 Score: 0.3313874429663904\n",
      "Confusion Matrix for each fold:\n",
      "Fold 1:\n",
      "[[141   1]\n",
      " [ 17   6]]\n",
      "\n",
      "Fold 2:\n",
      "[[136   1]\n",
      " [ 22   6]]\n",
      "\n",
      "Fold 3:\n",
      "[[141   3]\n",
      " [ 19   2]]\n",
      "\n",
      "Fold 4:\n",
      "[[150   0]\n",
      " [ 10   4]]\n",
      "\n",
      "Fold 5:\n",
      "[[148   0]\n",
      " [ 13   3]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import Lasso\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# Split data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=20)\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrices = []  # List to store confusion matrices for each fold\n",
    "\n",
    "# Adjust the threshold for binary classification\n",
    "threshold = 0.9 # Adjust this threshold as needed\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Apply SMOTE to the training fold only\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Train Lasso Regression model\n",
    "    lasso_model = Lasso(alpha=0.0005)  # Adjust regularization strength (alpha)\n",
    "    lasso_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict on validation fold\n",
    "    y_pred = lasso_model.predict(X_val_fold)\n",
    "    y_pred_binary = [1 if pred > threshold else 0 for pred in y_pred]\n",
    "\n",
    "    # Evaluate Lasso Regression model\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred_binary))\n",
    "    \n",
    "    # Calculate the confusion matrix for the fold\n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred_binary)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix for each fold:\")\n",
    "for i, conf_matrix in enumerate(conf_matrices):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(conf_matrix)\n",
    "    print()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347bcb3",
   "metadata": {},
   "source": [
    "XG boost with 5 fold cross validation and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "476b0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8368265416516408\n",
      "Average Precision: 0.4144263704163274\n",
      "Average Recall: 0.5059967671326837\n",
      "Average F1 Score: 0.4550085534736558\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "\n",
    "# 70-30 Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=3)  # Adjust hyperparameters as needed\n",
    "\n",
    "# Perform 5-fold cross-validation with SMOTE\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=16)  # Change the random state value here\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate XGBoost model\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312f8e4",
   "metadata": {},
   "source": [
    "after improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cccc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Average Accuracy: 0.9154597499425086\n",
      "Average Precision: 0.8841604202920079\n",
      "Average Recall: 0.9625857239447504\n",
      "Average F1 Score: 0.9193898941093561\n",
      "\n",
      "Confusion Matrix for Fold 1:\n",
      "[[193  11]\n",
      " [  9 194]]\n",
      "\n",
      "Confusion Matrix for Fold 2:\n",
      "[[186  18]\n",
      " [  6 197]]\n",
      "\n",
      "Confusion Matrix for Fold 3:\n",
      "[[188  15]\n",
      " [  7 197]]\n",
      "\n",
      "Confusion Matrix for Fold 4:\n",
      "[[177  26]\n",
      " [  8 196]]\n",
      "\n",
      "Confusion Matrix for Fold 5:\n",
      "[[185  18]\n",
      " [  2 201]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Features and target\n",
    "X = cleaned_data.drop('outcome', axis=1)\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# Scale features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=94)\n",
    "\n",
    "# XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Hyperparameter grid\n",
    "params = {'learning_rate': [0.05, 0.1, 0.2],\n",
    "          'max_depth': [3, 4, 5],\n",
    "          'n_estimators': [100, 150, 200],\n",
    "          'min_child_weight': [1, 2, 3],\n",
    "          'subsample': [0.8, 0.9, 1.0],\n",
    "          'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "          'gamma': [0, 0.5, 1]}\n",
    "\n",
    "# Randomized search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=100, scoring='accuracy', cv=5, verbose=2, random_state=94, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train XGBoost model with best hyperparameters\n",
    "xgb_model_selected = XGBClassifier(**best_params)\n",
    "\n",
    "# Get cross-validation predictions for each fold\n",
    "cv_predictions = cross_val_predict(xgb_model_selected, X_resampled, y_resampled, cv=5)\n",
    "\n",
    "# Collect confusion matrices for each fold\n",
    "conf_matrices = []\n",
    "for train_index, test_index in StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(X_resampled, y_resampled):\n",
    "    X_train_fold, X_test_fold = X_resampled[train_index], X_resampled[test_index]\n",
    "    y_train_fold, y_test_fold = y_resampled[train_index], y_resampled[test_index]\n",
    "    xgb_model_selected.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = xgb_model_selected.predict(X_test_fold)\n",
    "    conf_matrices.append(confusion_matrix(y_test_fold, y_pred_fold))\n",
    "\n",
    "# Print average metrics\n",
    "average_accuracy = np.mean(cross_val_score(xgb_model_selected, X_resampled, y_resampled, cv=5, scoring='accuracy'))\n",
    "average_precision = np.mean(cross_val_score(xgb_model_selected, X_resampled, y_resampled, cv=5, scoring='precision'))\n",
    "average_recall = np.mean(cross_val_score(xgb_model_selected, X_resampled, y_resampled, cv=5, scoring='recall'))\n",
    "average_f1 = np.mean(cross_val_score(xgb_model_selected, X_resampled, y_resampled, cv=5, scoring='f1'))\n",
    "\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall:\", average_recall)\n",
    "print(\"Average F1 Score:\", average_f1)\n",
    "\n",
    "# Print confusion matrices for each fold\n",
    "for i, conf_matrix in enumerate(conf_matrices):\n",
    "    print(f\"\\nConfusion Matrix for Fold {i+1}:\")\n",
    "    print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb7769",
   "metadata": {},
   "source": [
    "SVM with 5 fold cross validation and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3412df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8428092318788316\n",
      "Average Precision: 0.4237308429118774\n",
      "Average Recall: 0.4348107688272912\n",
      "Average F1 Score: 0.4266194467146344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# 70-30 Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  # Adjust hyperparameters as needed\n",
    "\n",
    "# Perform 5-fold cross-validation with SMOTE\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=16)  # Change the random state value here\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train SVM model\n",
    "    svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "\n",
    "    # Evaluate SVM model\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e131213",
   "metadata": {},
   "source": [
    "after improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f777aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Average Accuracy: 0.9783699059561128\n",
      "Average Precision: 0.9839988274595853\n",
      "Average Recall: 0.9724282816574906\n",
      "Average F1 Score: 0.9781148134141148\n",
      "\n",
      "Confusion Matrix for Fold 1:\n",
      "[[160  44]\n",
      " [ 26 177]]\n",
      "\n",
      "Confusion Matrix for Fold 2:\n",
      "[[161  43]\n",
      " [ 25 178]]\n",
      "\n",
      "Confusion Matrix for Fold 3:\n",
      "[[170  33]\n",
      " [ 28 176]]\n",
      "\n",
      "Confusion Matrix for Fold 4:\n",
      "[[160  43]\n",
      " [ 37 167]]\n",
      "\n",
      "Confusion Matrix for Fold 5:\n",
      "[[158  45]\n",
      " [ 23 180]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Features and target\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD',\n",
    "                  'heart rate', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2',\n",
    "                  'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium',\n",
    "                  'Urea nitrogen', 'Anion gap', 'Lactic acid','Blood sodium','MCH','RBC','Creatine kinase','PCO2','NT-proBNP']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# Scale features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=94)\n",
    "\n",
    "# SVM classifier\n",
    "svm_model = SVC()\n",
    "\n",
    "# Hyperparameter grid\n",
    "params = {'C': [0.1, 1, 10, 100, 1000, 10000],  # Expanded range\n",
    "          'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],  # Expanded range\n",
    "          'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "# Randomized search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(svm_model, param_distributions=params, n_iter=100, scoring='accuracy', cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get cross-validation results for accuracy, precision, recall, and F1 score\n",
    "cv_accuracy = cross_val_score(random_search.best_estimator_, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(random_search.best_estimator_, X_resampled, y_resampled, cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(random_search.best_estimator_, X_resampled, y_resampled, cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(random_search.best_estimator_, X_resampled, y_resampled, cv=5, scoring='f1')\n",
    "\n",
    "# Calculate average metrics\n",
    "average_accuracy = np.mean(cv_accuracy)\n",
    "average_precision = np.mean(cv_precision)\n",
    "average_recall = np.mean(cv_recall)\n",
    "average_f1 = np.mean(cv_f1)\n",
    "\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall:\", average_recall)\n",
    "print(\"Average F1 Score:\", average_f1)\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store confusion matrices\n",
    "conf_matrices = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    X_train_fold, X_test_fold = X_resampled[train_index], X_resampled[test_index]\n",
    "    y_train_fold, y_test_fold = y_resampled[train_index], y_resampled[test_index]\n",
    "\n",
    "    # Fit SVM model\n",
    "    svm_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_fold = svm_model.predict(X_test_fold)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix_fold = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "    conf_matrices.append(conf_matrix_fold)\n",
    "\n",
    "    # Print confusion matrix for this fold\n",
    "    print(f\"\\nConfusion Matrix for Fold {len(conf_matrices)}:\")\n",
    "    print(conf_matrix_fold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a8e83",
   "metadata": {},
   "source": [
    "KNN with 5 fold cross validation and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b84e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Evaluation Scores (5-Fold Cross Validation with SMOTE and K-Nearest Neighbors):\n",
      "Average Accuracy: 0.6822574828705373\n",
      "Average Precision: 0.2274026922409127\n",
      "Average Recall: 0.54375\n",
      "Average F1 Score: 0.3196947744905255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID', 'gendera'], axis=1).dropna(subset=['heart rate']).fillna(data.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = cleaned_data.drop(['outcome'], axis=1)\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# 70-30 Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select top k features\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation with SMOTE\n",
    "for train_index, val_index in kf.split(X_selected, y):\n",
    "    X_train_fold, X_val = X_selected[train_index], X_selected[val_index]\n",
    "    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Apply SMOTE to balance the classes only on the training fold\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Initialize KNeighborsClassifier with adjusted hyperparameters\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=15, weights='distance', algorithm='auto')\n",
    "\n",
    "    # Train KNeighborsClassifier on the resampled training data\n",
    "    knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict on validation data\n",
    "    y_pred_val = knn_model.predict(X_val)\n",
    "\n",
    "    # Evaluate KNeighborsClassifier on validation data\n",
    "    accuracy_scores.append(accuracy_score(y_val, y_pred_val))\n",
    "    precision_scores.append(precision_score(y_val, y_pred_val))\n",
    "    recall_scores.append(recall_score(y_val, y_pred_val))\n",
    "    f1_scores.append(f1_score(y_val, y_pred_val))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Print average evaluation scores\n",
    "print(\"\\nAverage Evaluation Scores (5-Fold Cross Validation with SMOTE and K-Nearest Neighbors):\")\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1 Score:\", avg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e674e",
   "metadata": {},
   "source": [
    "after improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46eb042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Fold 1:\n",
      "[[139   4]\n",
      " [ 18   4]]\n",
      "\n",
      "Confusion Matrix - Fold 2:\n",
      "[[139   4]\n",
      " [ 18   4]]\n",
      "\n",
      "Confusion Matrix - Fold 3:\n",
      "[[140   3]\n",
      " [ 19   3]]\n",
      "\n",
      "Confusion Matrix - Fold 4:\n",
      "[[139   3]\n",
      " [ 19   3]]\n",
      "\n",
      "Confusion Matrix - Fold 5:\n",
      "[[141   1]\n",
      " [ 16   6]]\n",
      "\n",
      "\n",
      "Average Evaluation Scores (5-Fold Cross Validation with SMOTE and K-Nearest Neighbors with Threshold):\n",
      "Average Accuracy: 0.8724390243902439\n",
      "Average Precision: 0.5714285714285714\n",
      "Average Recall: 0.18181818181818182\n",
      "Average F1 Score: 0.27513957307060755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID', 'gendera'], axis=1).dropna(subset=['heart rate']).fillna(data.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = cleaned_data.drop(['outcome'], axis=1)\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=22)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=73)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Define threshold\n",
    "threshold = 0.9  # Adjust the threshold as needed\n",
    "\n",
    "# Perform cross-validation with SMOTE\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train), 1):\n",
    "    X_train_fold, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Apply SMOTE to balance the classes only on the training data of each fold\n",
    "    smote = SMOTE(random_state=72)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Initialize KNeighborsClassifier with adjusted hyperparameters\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=9, weights='distance', algorithm='auto')\n",
    "\n",
    "    # Train KNeighborsClassifier\n",
    "    knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict probabilities on validation data\n",
    "    y_prob_knn = knn_model.predict_proba(X_val)[:, 1]  # Probability of positive class\n",
    "\n",
    "    # Apply threshold\n",
    "    y_pred_knn = (y_prob_knn > threshold).astype(int)\n",
    "\n",
    "    # Evaluate KNeighborsClassifier on validation data\n",
    "    accuracy_scores.append(accuracy_score(y_val, y_pred_knn))\n",
    "    precision_scores.append(precision_score(y_val, y_pred_knn))\n",
    "    recall_scores.append(recall_score(y_val, y_pred_knn))\n",
    "    f1_scores.append(f1_score(y_val, y_pred_knn))\n",
    "    \n",
    "    # Calculate confusion matrix for this fold\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred_knn)\n",
    "    print(f\"Confusion Matrix - Fold {i}:\")\n",
    "    print(conf_matrix)\n",
    "    print()\n",
    "\n",
    "# Calculate average scores\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print average evaluation scores\n",
    "print(\"\\nAverage Evaluation Scores (5-Fold Cross Validation with SMOTE and K-Nearest Neighbors with Threshold):\")\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1 Score:\", avg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f1f5e",
   "metadata": {},
   "source": [
    "Random forest with 5 fold cross validation and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e9e0a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.6847926433465561\n",
      "Average Precision: 0.2694809739764413\n",
      "Average Recall: 0.7799705965421937\n",
      "Average F1 Score: 0.39964875071258044\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load data (replace path with your actual data location)\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID'], axis=1)\n",
    "cleaned_data['gendera'] = data['gendera'] - 1\n",
    "cleaned_data = cleaned_data.dropna(subset=['heart rate'])\n",
    "cleaned_data = cleaned_data.fillna(cleaned_data.mean())\n",
    "\n",
    "# Feature selection\n",
    "X = cleaned_data[['age', 'hypertensive', 'Hyperlipemia', 'diabetes', 'Renal failure', 'COPD', \n",
    "                'heart rate','Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'SP O2', \n",
    "                'Urine output', 'MCV', 'RDW', 'Platelets', 'Lymphocyte', 'Blood calcium', \n",
    "                'Urea nitrogen', 'Anion gap', 'Lactic acid']]\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# 70-30 Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize Random Forest model with adjusted hyperparameters\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation with SMOTE\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=16)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Threshold for classification\n",
    "threshold = 0.3\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict probabilities on test data\n",
    "    y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Apply threshold for classification\n",
    "    y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    # Evaluate Random Forest model\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da21964",
   "metadata": {},
   "source": [
    "after improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be71a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Fold 1:\n",
      "[[145   0]\n",
      " [ 16   4]]\n",
      "\n",
      "Confusion Matrix - Fold 2:\n",
      "[[143   1]\n",
      " [ 18   3]]\n",
      "\n",
      "Confusion Matrix - Fold 3:\n",
      "[[143   1]\n",
      " [ 16   5]]\n",
      "\n",
      "Confusion Matrix - Fold 4:\n",
      "[[141   3]\n",
      " [ 19   1]]\n",
      "\n",
      "Confusion Matrix - Fold 5:\n",
      "[[143   1]\n",
      " [ 16   4]]\n",
      "\n",
      "\n",
      "Average Evaluation Scores (5-Fold Cross Validation with SMOTE and Random Forest with Threshold):\n",
      "Average Accuracy: 0.8894087213599409\n",
      "Average Precision: 0.7266666666666668\n",
      "Average Recall: 0.16619047619047617\n",
      "Average F1 Score: 0.26940740740740743\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(r'C:\\Users\\harsh\\OneDrive\\Desktop\\machine learning\\machine learning\\hospital dataset\\data01.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "cleaned_data = data.drop(['group', 'ID', 'gendera'], axis=1).dropna(subset=['heart rate']).fillna(data.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = cleaned_data.drop(['outcome'], axis=1)\n",
    "y = cleaned_data['outcome']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform 70:30 split for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Define threshold\n",
    "threshold = 0.6 # Adjust the threshold as needed\n",
    "\n",
    "# Perform cross-validation with SMOTE\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_train, y_train), 1):\n",
    "    X_train_cv, X_val = X_train[train_index], X_train[test_index]\n",
    "    y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Apply SMOTE to balance the classes only on the training data\n",
    "    smote = SMOTE(random_state=84)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_cv, y_train_cv)\n",
    "\n",
    "    # Initialize RandomForestClassifier with specified hyperparameters\n",
    "    rf_model = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=93)\n",
    "\n",
    "    # Train RandomForestClassifier\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predict probabilities on validation data\n",
    "    y_prob_rf = rf_model.predict_proba(X_val)[:, 1]  # Probability of positive class\n",
    "\n",
    "    # Apply threshold\n",
    "    y_pred_rf = (y_prob_rf > threshold).astype(int)\n",
    "\n",
    "    # Evaluate RandomForestClassifier\n",
    "    accuracy_scores.append(accuracy_score(y_val, y_pred_rf))\n",
    "    precision_scores.append(precision_score(y_val, y_pred_rf))\n",
    "    recall_scores.append(recall_score(y_val, y_pred_rf))\n",
    "    f1_scores.append(f1_score(y_val, y_pred_rf))\n",
    "    \n",
    "    # Calculate confusion matrix for this fold\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred_rf)\n",
    "    print(f\"Confusion Matrix - Fold {i}:\")\n",
    "    print(conf_matrix)\n",
    "    print()\n",
    "\n",
    "# Calculate average scores\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print average evaluation scores\n",
    "print(\"\\nAverage Evaluation Scores (5-Fold Cross Validation with SMOTE and Random Forest with Threshold):\")\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1 Score:\", avg_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
